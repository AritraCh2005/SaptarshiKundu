name: Evaluate Model Submission

on:
  pull_request:
    types: [opened, synchronize, reopened]
    paths:
      - 'submissions/**'
      
  workflow_dispatch:
    inputs:
      username:
        description: 'GitHub username of the submission'
        required: true
        type: string

  push:
    branches:
      - main
    paths:
      - 'submissions/**'

jobs:
  # Only runs on PR events to validate submission structure
  validate-submission:
    if: github.event_name == 'pull_request'
    runs-on: ubuntu-latest
    outputs:
      username: ${{ steps.extract-info.outputs.username }}
      valid: ${{ steps.validate.outputs.valid }}
    steps:
      - uses: actions/checkout@v3
      
      - name: Extract submission info
        id: extract-info
        run: |
          PR_FILES=$(gh pr view ${{ github.event.pull_request.number }} --json files -q '.files[].path')
          USERNAME=$(echo "$PR_FILES" | grep -oP 'submissions/\K[^/]+' | head -1)
          echo "username=$USERNAME" >> $GITHUB_OUTPUT
        env:
          GH_TOKEN: ${{ github.token }}
      
      - name: Validate submission structure
        id: validate
        run: |
          USERNAME="${{ steps.extract-info.outputs.username }}"
          # Check if submission directory exists 
          if [ ! -d "submissions/$USERNAME" ]; then
            echo "::error::Invalid submission structure. Directory submissions/$USERNAME not found."
            echo "valid=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Check if model file exists
          MODEL_FILE=$(find "submissions/$USERNAME" -type f -name "model.*" | head -1)
          if [ -z "$MODEL_FILE" ]; then
            echo "::error::Model file not found in submissions/$USERNAME/"
            echo "valid=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Check model file extension
          if [[ ! "$MODEL_FILE" =~ \.(pt|pth|h5|pb|saved_model|tflite)$ ]]; then
            echo "::error::Invalid model format. Supported formats: .pt, .pth, .h5, .pb, .saved_model, .tflite"
            echo "valid=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Check if metadata.json exists
          if [ ! -f "submissions/$USERNAME/metadata.json" ]; then
            echo "::error::metadata.json not found in submissions/$USERNAME/"
            echo "valid=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # Validate metadata.json format
          jq . "submissions/$USERNAME/metadata.json" > /dev/null 2>&1
          if [ $? -ne 0 ]; then
            echo "::error::Invalid metadata.json format"
            echo "valid=false" >> $GITHUB_OUTPUT
            exit 1
          fi
          
          # All checks passed
          echo "valid=true" >> $GITHUB_OUTPUT
          
  # Only runs on push (merged PR) or workflow_dispatch events
  evaluate-model:
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    outputs:
      username: ${{ steps.get-username.outputs.username }}
      model_size: ${{ steps.evaluate.outputs.model_size }}
      latency: ${{ steps.evaluate.outputs.latency }}
      accuracy: ${{ steps.evaluate.outputs.accuracy }}
      total_score: ${{ steps.evaluate.outputs.total_score }}
    steps:
      - uses: actions/checkout@v3
      
      - name: Get username
        id: get-username
        run: |
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            USERNAME="${{ github.event.inputs.username }}"
          else
            # For push events, detect the username from the commit files
            CHANGED_FILES=$(git diff-tree --no-commit-id --name-only -r ${{ github.sha }})
            USERNAME=$(echo "$CHANGED_FILES" | grep -oP 'submissions/\K[^/]+' | head -1)
            # Default to baseline if detection fails
            if [ -z "$USERNAME" ]; then
              USERNAME="baseline"
            fi
          fi
          echo "username=$USERNAME" >> $GITHUB_OUTPUT
        
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install torch torchvision tensorflow kaggle numpy pandas matplotlib
          
      - name: Download validation dataset
        run: |
          # Setup authentication - Method 1 (using just the credentials file)
          mkdir -p ~/.kaggle
          echo "{\"username\":\"${{ secrets.KAGGLE_USERNAME }}\",\"key\":\"${{ secrets.KAGGLE_KEY }}\"}" > ~/.kaggle/kaggle.json
          chmod 600 ~/.kaggle/kaggle.json
          
          # Verify credentials file
          if [ ! -s ~/.kaggle/kaggle.json ]; then
            echo "::error::Kaggle credentials file is empty or not created"
            exit 1
          fi
          
          # Test API access before download
          echo "Testing Kaggle API access..."
          kaggle datasets list -s ${{ secrets.KAGGLE_DATASET }} || { echo "::error::Failed to access Kaggle API"; exit 1; }
          
          # Download dataset with error handling
          echo "Downloading dataset..."
          kaggle datasets download -d ${{ secrets.KAGGLE_DATASET }} -p ./data --unzip || { echo "::error::Failed to download dataset"; exit 1; }
          
          # Check if files were actually downloaded
          if [ ! "$(ls -A ./data)" ]; then
            echo "::error::Dataset directory is empty after download"
            exit 1
          fi
          
          echo "Dataset download completed successfully"
          
      - name: Evaluate model
        id: evaluate
        run: |
          USERNAME="${{ steps.get-username.outputs.username }}"
          python scripts/evaluate.py \
            --submission_dir "submissions/$USERNAME" \
            --data_dir "./data" \
            --baseline_size 44.7 \
            --baseline_latency 30 \
            --baseline_accuracy 40.00 \
            --output_file "evaluation_result.json"
          
          # Extract evaluation results
          MODEL_SIZE=$(jq -r '.model_size' evaluation_result.json)
          LATENCY=$(jq -r '.latency' evaluation_result.json)
          ACCURACY=$(jq -r '.accuracy' evaluation_result.json)
          TOTAL_SCORE=$(jq -r '.total_score' evaluation_result.json)
          
          # Set outputs
          echo "model_size=$MODEL_SIZE" >> $GITHUB_OUTPUT
          echo "latency=$LATENCY" >> $GITHUB_OUTPUT
          echo "accuracy=$ACCURACY" >> $GITHUB_OUTPUT
          echo "total_score=$TOTAL_SCORE" >> $GITHUB_OUTPUT
          
  # Only runs on push (merged PR) or workflow_dispatch events
  update-leaderboard:
    needs: evaluate-model
    if: github.event_name == 'push' || github.event_name == 'workflow_dispatch'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'
          
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pandas tabulate
          
      - name: Update leaderboard
        run: |
          USERNAME="${{ needs.evaluate-model.outputs.username }}"
          MODEL_SIZE="${{ needs.evaluate-model.outputs.model_size }}"
          LATENCY="${{ needs.evaluate-model.outputs.latency }}"
          ACCURACY="${{ needs.evaluate-model.outputs.accuracy }}"
          TOTAL_SCORE="${{ needs.evaluate-model.outputs.total_score }}"
          SUBMISSION_DATE=$(date -u "+%Y-%m-%d %H:%M:%S UTC")
          
          python scripts/update_leaderboard.py \
            --username "$USERNAME" \
            --model_size "$MODEL_SIZE" \
            --latency "$LATENCY" \
            --accuracy "$ACCURACY" \
            --total_score "$TOTAL_SCORE" \
            --submission_date "$SUBMISSION_DATE" \
            --leaderboard_file "LEADERBOARD.md"
            
      - name: Commit and push leaderboard update
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"
          git add LEADERBOARD.md
          git commit -m "Update leaderboard with submission from ${{ needs.evaluate-model.outputs.username }}"
          git push
        env:
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN }}
          
  # Clean up model files after evaluation (only on push events)
  cleanup-model:
    needs: [evaluate-model, update-leaderboard]
    if: github.event_name == 'push'
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Delete model files
        run: |
          USERNAME="${{ needs.evaluate-model.outputs.username }}"
          if [ -d "submissions/$USERNAME" ] && [ "$USERNAME" != "baseline" ]; then
            MODEL_FILES=$(find "submissions/$USERNAME" -type f -name "model.*")
            if [ -n "$MODEL_FILES" ]; then
              for MODEL_FILE in $MODEL_FILES; do
                echo "Deleting model file: $MODEL_FILE"
                rm -f "$MODEL_FILE"
              done
              
              # Create a placeholder file to indicate model was deleted after evaluation
              echo "Model was evaluated and removed on $(date)" > "submissions/$USERNAME/model.evaluated"
              
              # Commit and push changes
              git config --local user.email "action@github.com"
              git config --local user.name "GitHub Action"
              git add "submissions/$USERNAME"
              git commit -m "Clean up model files for $USERNAME after evaluation"
              git push
            fi
          fi
        env:
          GITHUB_TOKEN: ${{ secrets.GH_TOKEN }} 